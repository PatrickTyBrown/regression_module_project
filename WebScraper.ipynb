{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscraper Notebook\n",
    "\n",
    "##### Order\n",
    "- Scrape Nasdaq IPO Calendar\n",
    "- Follow Links from Ininital Scrape and scrape company data\n",
    "- Search Company Stocks scrape first 180 days stock data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start By Importing what we will need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import pickle\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Functions that may be helpful later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle_file(obj, path):\n",
    "    file = open(path, 'wb')\n",
    "    pickle.dump(obj, file)\n",
    "    file.close()\n",
    "\n",
    "def load_pickle(path):\n",
    "    file = open(path, 'rb')\n",
    "    obj = pickle.load(file)\n",
    "    file.close()\n",
    "    return obj\n",
    "\n",
    "def start_driver(driver_path = './WebDriver/chromedriver'):\n",
    "    ser = Service(driver_path)\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    prefs = {\"profile.default_content_setting_values.notifications\" : 2}\n",
    "    chrome_options.add_experimental_option(\"prefs\",prefs)\n",
    "    \n",
    "    driver_path = './WebDriver/chromedriver'\n",
    "    driver = webdriver.Chrome(service = ser, options=chrome_options)\n",
    "    return driver\n",
    "\n",
    "#Loading a webpage\n",
    "# driver.get('https://www.nasdaq.com/market-activity/ipos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape Nasdaq IPO Calendar\n",
    "\n",
    "- Define Scrape Specific Functions\n",
    "  - Navigate To Starting Date\n",
    "  - Generate Dictionary from rows in table\n",
    "  - Navigate to next page\n",
    "- Define Main Loop\n",
    "  - Load Driver\n",
    "  - Iterate Through Months from starting date to end date\n",
    "- Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_starting_page(driver, starting_date = '01/1995'):\n",
    "    #Open Date \n",
    "    date_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((\n",
    "        By.XPATH, '/html/body/div[3]/div/main/div[2]/div[2]/div[2]/div/div[2]/div/div[2]/div[2]/button'))\n",
    "        ).click()\n",
    "\n",
    "    #Set Date Picker Date\n",
    "    date_picker = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((\n",
    "        By.XPATH, '/html/body/div[3]/div/main/div[2]/div[2]/div[2]/div/div[2]/div/div[2]/div[2]/div/input'))\n",
    "        )\n",
    "    driver.execute_script(f\"arguments[0].value = '{starting_date}';\", date_picker)     \n",
    "        # .setAttribute('value', '01/1999')\n",
    "\n",
    "    #Navigate to New Page\n",
    "    apply_date_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((\n",
    "        By.XPATH, '/html/body/div[3]/div/main/div[2]/div[2]/div[2]/div/div[2]/div/div[2]/div[2]/div/button[2]'))\n",
    "        ).click()\n",
    "\n",
    "def generate_data(driver):\n",
    "    table = driver.find_element(By.XPATH,\n",
    "        './/table',{'class':'market-calendar-table__table'})\n",
    "    html = table.get_attribute('innerHTML')\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    url_base = 'https://www.nasdaq.com'\n",
    "    table_data = []\n",
    "    # print(soup)\n",
    "    # if soup.find('th').text == '':\n",
    "    #     return []\n",
    "\n",
    "    rows = soup.find_all('tr',{'class':'market-calendar-table__row'})\n",
    "    for row in rows:\n",
    "        row_data = {}\n",
    "\n",
    "        th = row.find('th', {'role':'cell'})\n",
    "        ticker = th.findChild().text\n",
    "        if not ticker:\n",
    "            continue\n",
    "\n",
    "        row_data[th['data-column']] = ticker\n",
    "        row_data['link'] = url_base + row.find('a')['href']\n",
    "\n",
    "        cells = row.find_all('td',{'role':'cell'})        \n",
    "        for cell in cells:\n",
    "            row_data[cell['data-column']] = cell.findChild().text\n",
    "        table_data.append(row_data)\n",
    "    return table_data\n",
    "\n",
    "# generate_data(driver)\n",
    "\n",
    "def get_next_calendar_page(driver):\n",
    "    next_scroll = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((\n",
    "        By.XPATH, \".//button[@class='time-belt__next']\"))\n",
    "        ).click()\n",
    "\n",
    "    next_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((\n",
    "        By.XPATH, \".//button[@class='time-belt__item']\"))\n",
    "        ).click()\n",
    "\n",
    "# get_next_calendar_page(driver)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_60201/412141742.py:12: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(driver_path, options=chrome_options)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started Driver\n",
      "Loaded Initial Page\n",
      "Loaded Starting Page\n",
      "Checkpoint-2023\t(Total: 302;\t Len: 7630)\n",
      "All Done\n"
     ]
    }
   ],
   "source": [
    "def main_loop(pickup_file = False, starting_date= '01/1995', delay = 10):\n",
    "    if pickup_file:\n",
    "        data = load_pickle(pickup_file)\n",
    "    else:\n",
    "        data = []\n",
    "\n",
    "    driver = start_driver('./WebDriver/chromedriver')\n",
    "    print('Started Driver')\n",
    "\n",
    "    time.sleep(3)\n",
    "\n",
    "    driver.get('https://www.nasdaq.com/market-activity/ipos')\n",
    "    print('Loaded Initial Page')\n",
    "\n",
    "    # time.sleep(delay)\n",
    "\n",
    "    get_starting_page(driver, starting_date=starting_date)\n",
    "    print('Loaded Starting Page')\n",
    "\n",
    "    date_year = 0\n",
    "    count = 0\n",
    "    while date_year <= 2022:\n",
    "        count+=1\n",
    "        date_year = int(driver.find_element(By.XPATH, \".//button[@class='time-belt__item']\").get_attribute('data-year'))\n",
    "\n",
    "        data += generate_data(driver)\n",
    "        pickle_file(data, f'Data/nasdaq_checkpoint_{date_year}')\n",
    "        print(f'Checkpoint-{date_year}\\t(Navigations: {count};\\t Rows of Data: {len(data)})',end='\\r', flush=True)\n",
    "        time.sleep(random.randint(10,30))\n",
    "\n",
    "        get_next_calendar_page(driver)\n",
    "    \n",
    "    print('\\nAll Done')\n",
    "    return data\n",
    "\n",
    "data = main_loop(starting_date='02/1998', pickup_file='Data/nasdaq_checkpoint_1998')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Follow Links from Ininital Scrape and scrape company data\n",
    "\n",
    "- Define Scrape Specific Functions\n",
    "  - Load Data From Initial Scrape\n",
    "  - Get Page from link in data\n",
    "  - Generate Dictionary for page data\n",
    "  - Navigate to next page (Row in data)\n",
    "- Define Main Loop\n",
    "- Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_company_overview_data(driver, data={}):\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    table_soup = soup.find('table')\n",
    "    table_rows = table_soup.find_all('tr')\n",
    "    for row in table_rows:\n",
    "        data[row.find('th').text] = row.find('td').text\n",
    "    return data\n",
    "\n",
    "def generate_company_financials_data(driver, data={}):\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    table_soup = soup.find('table')\n",
    "    table_rows = table_soup.find_all('tr')\n",
    "    for row in table_rows:\n",
    "        data[row.find('th').text] = row.find('td').text\n",
    "    filing_table = soup.find_all('tbody')[1]\n",
    "    filing_row = filing_table.find('tr')\n",
    "    filing_cells = filing_row.find_all('td')\n",
    "    data['form_type'] = filing_cells[1].text\n",
    "    data['filing_date_received'] = filing_cells[2].text\n",
    "    data['filing_link'] = filing_cells[3].find('a')['href']\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = start_driver('./WebDriver/chromedriver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver = start_driver('./WebDriver/chromedriver')\n",
    "\n",
    "ipo_calendar_data = load_pickle('Data/nasdaq_ipos')\n",
    "\n",
    "# ipo_data[:5]\n",
    "\n",
    "data = []\n",
    "row_data = {}\n",
    "ticker = ipo_calendar_data[0]['proposedTickerSymbol']\n",
    "row_data ['proposedTickerSymbol']= ticker\n",
    "url = ipo_calendar_data[0]['link']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(ipo_calendar_data[0]['link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.nasdaq.com/market-activity/ipos/financial-filings?dealId=1205111-102356'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def navigate_to_financials(driver, url):\n",
    "    financial_url = url.replace('overview','financial-filings')\n",
    "    driver.get(financial_url)\n",
    "    return financial_url\n",
    "\n",
    "navigate_to_financials(driver, url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Revenue': '$145,000.00',\n",
       " 'Net Income': '-$8,518,000.00',\n",
       " 'Total Assets': '$6,031,000.00',\n",
       " 'Total Liabilities': '$2,156,000.00',\n",
       " \"Stockholders' Equity\": '$3,875,000.00',\n",
       " 'form_type': 'F-1',\n",
       " 'filing_date_received': '03/07/2022',\n",
       " 'filing_link': 'http://secfilings.nasdaq.com/filingFrameset.asp?FilingID=15638250&View=html'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_company_financials_data(driver, data={}):\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    table_soup = soup.find('table')\n",
    "    table_rows = table_soup.find_all('tr')\n",
    "    for row in table_rows:\n",
    "        data[row.find('th').text] = row.find('td').text\n",
    "    filing_table = soup.find_all('tbody')[1]\n",
    "    filing_row = filing_table.find('tr')\n",
    "    filing_cells = filing_row.find_all('td')\n",
    "    data['form_type'] = filing_cells[1].text\n",
    "    data['filing_date_received'] = filing_cells[2].text\n",
    "    data['filing_link'] = filing_cells[3].find('a')['href']\n",
    "    return data\n",
    "\n",
    "generate_company_financials_data(driver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Company Stocks scrape first 180 days stock data\n",
    "\n",
    "- Define Scrape Specific Functions\n",
    "  - Load Data From Initial Scrape\n",
    "  - Generate Search Url for ticker\n",
    "  - Check if search has valid results\n",
    "  - Generate Dictionary for page data\n",
    "  - Navigate to next\n",
    "- Define Main Loop\n",
    "- Run"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b834e98849dd830995cef45fef40061e933db58847d4cb56560919edabbced90"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('regression')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
